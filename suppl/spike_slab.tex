\documentclass[10pt, a4paper,openany]{report}

%% Load packages %%
%\usepackage[applemac]{inputenx}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}     % Mathematical writing
\usepackage{amssymb}


%% Load personalised commands %%
\input{utils.tex}


\begin{document}

\section{Hard sparsity prior: spike and Slab}
%The variational distribution over sparse weights has a factorial nature: it can be written as a mixture of $2^M$ components where $M$ is the number of weights.
%EXPLAIN SOME THEORY

\subsection{The model}
We have a data set $\bfY$ of $M$ input views with dimensionality $\bfY^m \in \R^{N \times D_m}$. The sparse group factor analysis model is defined as follows:
\begin{align}
&y_{nd} \approx \Ndist{y_{nd}^m}{\bfw_{d,:}^m\bfz_{n,:},1/\tau_d^m} \\
&w_{dk}^m \approx \theta\Ndist{w_{dk}}{0,1/\alpha_k^m} + (1-\theta)\delta_0(w_{dk}^m)\\
&z_{nk} \approx \Ndist{z_{nk}}{0,1}
\end{align}

\subsection{Efficient variational inference}
The presence of the Dirac delta mass function makes the application of variational inference troublesome. However, there exists a simple reparameterization of the spike and slab prior that is more amenable to approximate inference.\\
Assume a Gaussian random variable $w \approx \Ndist{\hat{w}}{0,\sigma^2}$ and a Bernoulli random variable $s \approx \pi^{s}(1-\pi)^{1-s}$. The product $s\hat{w}$ forms a new random variable distributed according to the original spike and slab prior. Thus, we can reparametrise $w=s\hat{w}$ and assign the prior distributions on $s$ and $\hat{w}$. Thus, the reparametrised spike and slab takes the form:
\[
p(\hat{w},s) = \Ndist{\hat{w}}{0,\sigma^2} \pi^{s}(1-\pi)^{1-s}
\]
A simple approach for variational inference is to consider the mean-field approximation:
\[
q(\hat{w},s) = q(\hat{w})q(s)
\]
which leads to easy updates. However, each pair of variables $\{s\hat{w}\}$ is strongly correlated since their product is the underlying variable that interacts with the data. Therefore, considering a unimodal distribution with the variables $\hat{w}$ and $s$ being independent leads toa  very inefficient inference. Instead, we introduce a paired mean field approximation $q(w,s)$ which approximates better the factorial nature of the true posterior.
\[
q(\hat{\bfW},\bfS) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m,s_{dk}^m)
\]
Joint probability density function:
\begin{align*}
p(\bfY,\hat{\bfW},\bfS,\bfZ) = &\prod_{m=1}^{M} \prod_{n=1}^{N} \prod_{d=1}^{D_m} \Ndist{y_{nd}^m}{\sum_{k=1}^{K} s_{dk}^m \hat{w}_{dk}^m z_{nk},1/\tau_d} \times \\
&\prod_{d=1}^{D_m}\prod_{k=1}^{K} \Ndist{\hat{w}_{dk}^m}{0,1/\alpha_k^m} \theta^{s_{dk}^m}(1-\theta)^{1-s_{dk}} \times \\
&\prod_{n=1}^{N}\prod_{k=1}^{K} \Ndist{z_{nk}}{0,1}
\end{align*}
%Log Marginal likelihood:
%\[
%\log p(\bfY) = \log \sum_{\bfS} \int_{\bfW,\bfZ} p(\bfY,\hat{\bfW},\bfS,\bfZ) d\bfW d\bfZ
%\]
%Evidence lower bound on the log marginal likelihood:
%\[
%\bfF = \sum_{\bfS} \int_{\bfW,\bfZ} q(\hat{\bfW},\bfS,\bfZ) \log \frac{p(\bfY,\hat{\bfW},\bfS,\bfZ)}{q(\hat{\bfW},\bfS,\bfZ)} d\bfW d\bfZ
%\]
Full variational distribution:
\[
q(\hat{\bfW},\bfS,\bfZ) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m,s_{dk}^m) \prod_{k=1}^{K}\prod_{n=1}^{N} q(z_{n,k})
\]

\subsection{Derivation of update equations}
The optimal distribution $\hat{q}_i$ for each variable $\bfx_i$, is the following:
\begin{equation} \label{eq:optimal_q_dist}
\log \hat{q}_i(\bfx_i) = \E_{i \neq j} [\log p(\bfY,\bfX)] + \const
\end{equation}
where $\E_{i \neq j}$ denotes an expectation with respect to the $q$ distributions over all variables $\bfx_j$ except for $\bfx_i$.
The additive constant is set by normalising the distribution $\hat{q}_i(\bfz_i)$:
\[
\hat{q}_i(\bfx_i) = \frac{\exp(\E_{i \neq j}[\log p(\bfY,\bfX)])}{\int \exp(\E_{i \neq j}[\log p(\bfY,\bfX)]) d\bfX}
\]

\subsection*{Latent variables}
Term from the likelihood $p(\bfY|\hat{\bfW},\bfZ,\btau,\bfS)$:
%$\sum_{n=1}^{N}\sum_{d=1}^{D} \Ndist{y_{n,d}}{\sum_{k=1}^{K}\hat{w}_{dk}s_{dk}z_{nk},\tau_d}$:
\begin{align*}
&\sum_{m=1}^{M} \sum_{d=1}^{D=m} \la\tau_d^m\ra \la s_{dk}^m \hat{w}_{dk}^m \ra y_{nd}^m z_{nk}
-\frac{1}{2}\sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_d^m\ra \la (s_{dk}^m\hat{w}_{dk}^m)^2 \ra z_{nk}^2 \\
&-\frac{1}{2}\sum_{m=1}^{M} \frac{1}{2}\sum_{d=1}^{D} \la\tau_d^m\ra \sum_{j \neq k} (\la s_{dj}^m \hat{w}^m_{dj} \ra \la z_{nj} \ra) \la\hat{w}^m_{dk} s_{dk}^m \ra \la z_{nk} \ra + \const
\end{align*}
Term from the prior $p(z_{nk})$:
\[
-\frac{1}{2} z_{nk}^2 + \const
\]
Variational distribution:
\[
q(\bfZ) = \prod_{k=1}^{K} \prod_{n=1}^{N} q(z_{nk}) = \prod_{k=1}^{K} \prod_{n=1}^{N} \Ndist{z_{nk}}{\mu_{z_{nk}},\sigma_{z_{nk}}}
\]
where
\[
\sigma_{z_{nk}}^2 = \Big( \sum_{m=1}^{M} \sum_{d=1}^{D_m} \tau_d^m \la (s_{dk}^m\hat{w}_{dk}^m)^2 \ra + 1 \Big)^{-1}
\]
\[
\mu_{z_{nk}} = \sigma_{z_{nk}}^2 \sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_d^m\ra \la s_{dk}^m\hat{w}_{dk}^m \ra \Big( y_{nd}^m - \sum_{j \neq k} \la s_{dj}^m \hat{w}_{dj}^m \ra \la z_{nj} \ra \Big)
\]

\subsection*{Spike and Slab Weights}
Variational distribution:
\[
q(\hat{\bfW},\bfS) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m,s_{dk}^m) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m|s_{dk}^m)q(s_{dk}^m)
\]
Update for $q(s_{dk}^m)$:
\[
\gamma_{dk} = q(s_{dk}=1) = \frac{1}{1+\exp(-\lambda_{dk})}
\]
where
\begin{align*}
\lambda_{dk}^m = \la \log\frac{\theta}{1-\theta} \ra + 0.5\log\frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} - 0.5\log\Big( \sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} \Big) \\
+ \frac{\la\tau_d^m\ra}{2} \frac{ \Big( \sum_{n=1}^{N} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m\ra \sum_{n=1}^{N} \la z_{nk} \ra \la z_{nj} \ra \Big)^2} {\sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\end{align*}
Update for $q(\hat{w}_{dk}^m)$:
\begin{align*}
q(\hat{w}_{dk}^m|s_{dk}^m=0) = \Ndist{\hat{w}_{dk}^m}{0,1/\alpha_k^m} \\
q(\hat{w}_{dk}^m|s_{dk}^m=1) = \Ndist{\hat{w}_{dk}^m}{\mu_{w_{dk}^m},\sigma_{w_{dk}^m}^2}
\end{align*}
where
\[
\mu_{w_{dk}^m} = \frac{ \sum_{n=1}^{N} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m \ra \sum_{n=1}^{N} \la z_{nk} \ra \la z_{nj} \ra } { \sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\]
\[
\sigma_{w_{dk}^m} = \frac{ \la\tau_d^m\ra^{-1} } { \sum_{n=1}^{N} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\]
Taken together this means that we can update $q(\hat{w}_{dk}^m,s_{dk}^m)$ using:
\[
q(\hat{w}_{dk}^m|s_{dk}^m) \times q(s_{dk}^m) = \Ndist{ \hat{w}_{dk}^m } { s_{dk}^m \mu_{w_{dk}^m}, s_{dk}^m\sigma_{w_{dk}^m}^2 + (1-s_{dk}^m)/\alpha_k^m} \quad \times \quad (\lambda_{dk}^m)^{s_{dk}^m} (1-\lambda_{dk}^m)^{1-s_{dk}}
\]


\subsection*{ARD precision (alpha)}
\begin{equation} \label{eq:optimal_q_dist}
\log \hat{q}_i(\bfx_i) = \E_{i \neq j} [\log p(\bfY,\bfX)] + \const
\end{equation}
Term from the prior $\log p(\alpha_k^m)$:
\[
(a_0^\alpha-1) \log(\alpha_k^m) - b_0^{\alpha}\alpha_k^m + \const
\]
Term from the prior $\log p(\bfw_{:k}^m) = \sum_{d=1}^{D_m} \log p(s_{dk}^m,\hat{w}_{dk}^m)$:
\[
\frac{D_m}{2}\log(\alpha_k^m) -\frac{\alpha_k^m}{2}\sum_{d=1}^{D} \la \hat{w}_{dk}^2 \ra + \sum_{d=1}^{D_m} \{ \la s_{dk}^m \ra \log\theta_0 + (1-\la s_{dk}^m \ra) \log(1-\theta_0)  \} + \const
\]
Writing everything together:
\[
\Big(a_0^{\alpha} + \frac{D_m}{2} - 1 \Big)\log\alpha_k^m - \Big(b_0^{\alpha} + \frac{1}{2} \sum_{d=1}^{D_m}\la (\hat{w}_{d,k}^m)^2 \ra \Big) \alpha_k^m + \const
\]
Variational distribution:
\[
q(\balpha) = \prod_{m=1}^{M} \prod_{k=1}^{K} \mathcal{G}(\alpha_k^m | \hat{a}_{mk}^{\alpha}, \hat{b}_{mk}^{\alpha})
\]
where
\[
\hat{a}_{mk}^\alpha = a_0^\alpha + \frac{D_m}{2}
\]
\[
\hat{b}_{mk}^\alpha = b_0^\alpha +\frac{ \sum_{d=1}^{D_m} \la (\hat{w}_{d,k}^m)^2 \ra }{2}
\]

\subsection*{Noise precision (tau)}
Term from the prior $p(\tau_d^m)$:
\[
(a_0^\tau - 1) \log \tau_d^m - b_0^\tau \tau_d^m + \const
\]
Term from the likelihood $\Ndist{y_{n,d}^m}{\sum_{k=1}^{K}\hat{w}_{dk}^m s_{dk}^m z_{nk},\tau_d^m}$:
\[
\frac{N}{2}\log \tau_d^m - \frac{\tau_d^m}{2} \sum_{n=1}^{N} \la (y_{nd}^m - \sum_k^{K} w_{dk}^m s_{dk}^m z_{nk})^2 \ra + \const
\]
Rewriting everything together:
\[
\Big(a_0^\tau - 1 + \frac{N}{2} \Big)\log \tau_d^m - \Big( b_0 + \frac{1}{2} \sum_{n=1}^{N} \la (y_{nd}^m - \sum_k^{K} \hat{w}_{dk}^m s_{dk}^m z_{nk})^2 \ra  \Big )\tau_d^m
\]
Variational distribution:
\[
q(\btau) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} q(\tau_d^m) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \mathcal{G}(\tau_d^m|\hat{a}_{md}^{\tau},\hat{b}_{md}^{\tau})
\]
where
\[
\hat{a}_{md}^{\tau} = a_0^{\tau} + \frac{N}{2}
\]
\[
\hat{b}_{md}^{\tau} = b_0^{\tau} + \frac{1}{2} \sum_{n=1}^{N} \la (y_{nd}^m - \sum_k^{K} \hat{w}_{dk}^m s_{dk}^m z_{n,k})^2 \ra
\]

\subsection*{Spike and Slab sparsity parameter $\theta$}
Unless a given factor is specifically annotated in a given view, the sparisty parameter $\theta^m_{k}$ of the Spike and Slab prior on $w_{k,d}^m, \forall d$ is given a Beta prior: $P(\theta_k^m) = \mathrm{Beta} (a_0, b_0)$. The postrerior $q(\theta_k^m)$ is Beta distributed and the update of its parameters $a_k^m$ and $b_k^m$ are given below:\\

\begin{equation*}
  \begin{aligned}
    &a_{k}^m = \sum_{d} \la S^m_{k,d}\ra + a_0\\
    &b_{k}^m = b_0 - \sum_{d} \la S^m_{k,d}\ra + D_m
  \end{aligned}
\end{equation*}

\subsection{Lower bound}

\subsubsection{Likelihood term}
%Matrix form:
%\[
%\]
Vector form:
% \[
% -\frac{DN}{2} \log(2\pi) + \frac{N}{2}\sum_{d=1}^{D}\log(\tau_d) - \frac{1}{2} \sum_{d=1}^{D} \big( \bfy_d - \sum_{k=1}^{K}\la s_{dk} \hat{w}_{dk} \ra \la \bfz_k \ra \big)^T \big( \tau_d \bfI \big) \big( \bfy_d^T - \sum_{k=1}^{K}\la s_{dk} \hat{w}_{dk} \ra \la \bfz_k \ra \big)
% \]
\[
-\sum_{m=1}^M \frac{ND_m}{2} \log(2\pi) + \frac{N}{2}\sum_{m=1}^{M}\sum_{d=1}^{D_m}\log(\tau_d^m) - \frac{1}{2} \sum_{m=1}^{M}\sum_{d=1}^{D_m} \big( \bfy_d^m - \sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la \bfz_k \ra \big)^T \big( \tau_d^m \bfI \big) \big( \bfy_d^m - \sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la \bfz_k \ra \big)
\]

Scalar form:
% \[
% -\frac{DN}{2} \log(2\pi) + \frac{N}{2}\sum_{d=1}^{D}\log(\la \tau_d \ra) - \sum_{d=1}^{D} \frac{\la \tau_d \ra}{2} \sum_{n=1}^{N} \big( y_{nd} - \sum_{k=1}^{K}\la s_{dk} \hat{w}_{dk} \ra \la z_{nk} \ra \big)^2
% \]
\[
-\sum_{m=1}^M \frac{ND_m}{2} \log(2\pi) + \frac{N}{2} \sum_{m=1}^M \sum_{d=1}^{D_m} \log(\la \tau_d^m \ra) -\sum_{m=1}^M \sum_{d=1}^{D_m} \frac{\la \tau_d^m \ra}{2} \sum_{n=1}^{N} \big( y_{nd}^m - \sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la z_{nk} \ra \big)^2
\]
Extending terms and rearranging:
% \begin{align*}
% &-\frac{DN}{2} \log(2\pi) + \frac{N}{2} \sum_{d=1}^{D}\log(\tau_d) - \frac{1}{2} \sum_{d=1}^{D} \tau_d \bfy_d^T \bfy_d\\
% &+ \sum_{k=1}^{K} \Big( \sum_{d=1}^{D} \tau_d \la s_{dk} \hat{w}_{dk} \ra \bfy_d \Big)^T \la \bfz_k \ra \\
% &- \frac{1}{2} \sum_{k=1}^{K} \Big( \sum_{d=1}^{D} \tau_d \la s_{dk} \hat{w}_{dk}^2 \ra \Big) \la \bfz_k^T \bfz_k \ra\\
% &- \sum_{k=1}^{K} \Big( \sum_{k'=k+1}^{K} \Big( \sum_{d=1}^{D} \tau_d \la s_{dk} \hat{w}_{dk} \ra \la s_{dk'} \hat{w}_{dk'} \ra \Big) \la \bfz_{k'} \ra \Big )^T \la \bfz_k \ra
% \end{align*}

\subsubsection{W and S terms}
$p(\hat{\bfW},\bfS)$:
\begin{align*}
-\sum_{m=1}^{M}\frac{KD_m}{2}\log(2\pi) + \sum_{m=1}^{M}\frac{D_m}{2}\sum_{k=1}^{K} \log(\alpha_k^m) - \sum_{m=1}^{M} \frac{\alpha_k^m}{2} \sum_{d=1}^{D_m} \sum_{k=1}^{K} \la (\hat{w}_{dk}^m)^2 \ra \\
+ \la \log(\theta) \ra \sum_{m=1}^{M} \sum_{d=1}^{D_m} \sum_{k=1}^{K} \la s_{dk}^m \ra + \la \log(1-\theta) \ra \sum_{m=1}^{M} \sum_{d=1}^{D_m}\sum_{k=1}^{K} (1- \la s_{dk}^m \ra)
\end{align*}
%$p(\hat{\bfW})$:
%\[
%-\frac{KD}{2}\log(2\pi) + \frac{D}{2}\sum_{k=1}^{K} \log(\alpha_k) - \frac{\alpha_k}{2} \sum_{d=1}^{D} \sum_{k=1}^{K} \la \hat{w}_{dk}^2 \ra
%\]
$q(\hat{\bfW},\bfS)$:
%\[
%\frac{DK}{2}\log(2\pi) + \frac{DK}{2}\log(\sigma_w^2) - \frac{1}{2} \log(\sigma_w^2) \sum_{d=1}^{D}\sum_{k=1}^{K} \la s_{dk} \ra + \frac{1}{2}  \sum_{d=1}^{D}\sum_{k=1}^{K} \la s_{dk} \ra \log(\sigma_w_{dk}^2)
%\]
\begin{align*}
-\sum_{m=1}^{M}\frac{KD_m}{2}\log(2\pi) + \frac{1}{2}\sum_{m=1}^{M}\sum_{d=1}^{D_m}\sum_{k=1}^{K}\log(\la s_{dk}^m \ra \sigma_{w_{dk}^m}^2 + (1-\la s_{dk}^m \ra)/\alpha_k^m) \\
+ \sum_{m=1}^{M} \sum_{d=1}^{D_m} \sum_{k=1}^{K} (1-\la s_{dk}^m \ra) \log(1 - \la s_{dk}^m \ra) - \la s_{dk}^m \ra \log \la s_{dk}^m \ra
\end{align*}
%$p(\bfS)$:
%\[
%\la \log(\theta) \ra \sum_{d=1}^{D}\sum_{k=1}^{K} \la s_{dk} \ra + \la \log(1-\theta) \ra \sum_{d=1}^{D}\sum_{k=1}^{K} (1- \la s_{dk} \ra)
%\]
%$q(\bfS)$:
%\[
%\sum_{d=1}^{D}\sum_{k=1}^{K} (1-\la s_{dk} \ra) \log(1 - \la s_{dk} \ra) - \la s_{dk} \ra \log \la s_{dk} \ra
%\]

\subsubsection{Z term}
\[
\mathbb{E} [\log P(\bfZ)] = -\frac{NK}{2}\log(2\pi) -\frac{1}{2} \sum_{n=1}^{N} \la z_{nk}^2 \ra
\]
\[
\mathbb{E} [\log q(\bfZ)] = - \frac{NK}{2}(1 + \log(2\pi)) - \frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^{K} \log(\sigma_{z_{nk}}^2)
\]

%\section{Variational inference}

\subsection*{alpha term}
\[
\mathbb{E} [\log p(\balpha)] = \sum_{m=1}^{M}\sum_{k=1}^{K}\Big(a_0^\alpha\log b_0^\alpha +   (a_0^\alpha - 1) \la \log \alpha_k \ra - b_0^\alpha \la \alpha_k \ra - \log \Gamma(a_0^\alpha) \Big)
\]
\[
\mathbb{E} [\log q(\balpha)] = \sum_{m=1}^{M}\sum_{k=1}^{K} \Big( \hat{a}_{k}^\alpha \log \hat{b}_{k}^\alpha + (\hat{a}_{k}^\alpha - 1) \la \log \alpha_k \ra - \hat{b}_{k}^\alpha \la \alpha_k \ra - \log \Gamma(\hat{a}_{k}^\alpha) \Big)
\]

\subsection*{tau}
%\[
%\mathbb{E} [\log P(\btau)] = \sum_{m=1}^{M}\sum_{d=1}^{Dm} a_0^\tau \log b_0^\tau + (a_0^\tau - 1) \la \log %\tau_d^m \ra - b_0^\tau \la \tau_d^m \ra - \log \Gamma(a_0^\tau)
%\]
\[
\mathbb{E} [\log P(\tau)] = \sum_{m=1}^{M} D_m a_0^\tau \log b_0^\tau + \sum_{m=1}^{M}\sum_{d=1}^{Dm} (a_0^\tau - 1) \la \log \tau_d^m \ra - \sum_{m=1}^{M}\sum_{d=1}^{Dm} b_0^\tau \la \tau_d^m \ra - \sum_{m=1}^{M} D_m \Gamma(a_0^\tau)
\]

\[
\mathbb{E} [\log Q(\btau)] = \sum_{m=1}^{M}\sum_{d=1}^{D_m} \Big( \hat{a}_{dm}^\tau \log \hat{b}_{dm}^\tau + (\hat{a}_{dm}^\tau - 1) \la \log \tau_d^m \ra - \hat{b}_{dm}^\tau \la \tau_d^m \ra - \log \Gamma(\hat{a}_{dm}^\tau) \Big)
\]

\subsection*{Theta}
\begin{equation*}
    \begin{aligned}
      &\mathbb{E}\left[ \log P(\theta) \right] = \sum_{m=1}^M \sum_{k=1}^K\sum_{d=1}^{D_m}\left( (a_0 - 1) \times \la \log(\pi^m_{d, k}) \ra + (b_0 -1) \la \log(1 - \pi^m_{d, k}) \ra - \log (\mathrm{B} (a_0, b_0))\right) \\
      &\mathbb{E}\left[ \log Q(\theta) \right] = \sum_{m=1}^M \sum_{k=1}^K\sum_{d=1}^{D_m}\left( (a^m_{k,d} - 1) \times \la \log(\pi^m_{d, k}) \ra + (b^m_{k,d} -1) \la \log(1 - \pi^m_{d, k}) \ra - \log (\mathrm{B} (a^m_{k,d}, b^m_{k,d})) \right) \\
    \end{aligned}
\end{equation*}


where the expectations are calculated as follows:

\subsection*{Expectations}
The expectations are calculated as follows:
\[
\la s_{dk}^m \hat{w}_{dk}^m \ra = \lambda_{dk}^m \mu_{w_{dk}^m}
\]
\[
\la s_{dk}^m \hat{w}_{dk}^{m2} \ra = \lambda_{dk}^m (\mu_{w_{dk}^m}^2 + \sigma_{w_{dk}^m}^2)
\]
\[
\la \hat{w}_{dk}^{m2} \ra = \lambda_{dk}^m(\mu_{w_{dk}^m}^2 + \sigma_{w_{dk}^m}^2) + (1-\lambda_{dk}^m)/\alpha_k^m
\]
%\[
%\la \bfz_k^T \bfz_k \ra = \mu_{\bfz_k}^T \mu_{\bfz_k} + \tr(\Sigma_{\bfz_{:,k}})
%\]
\[
\la z_{nk} \ra = \mu_{z_{nk}}
\]
\[
\la z_{nk}^2 \ra = \mu_{z_{nk}}^2 + \sigma_{z_{nk}}^2
\]
\[
\la \tau_d^m \ra = \tilde{a}_{md}^\tau / \tilde{b}_{md}^\tau
\]
\[
\la \log \tau_d^m \ra = \psi(\tilde{a}_{md}^\tau) - \log \tilde{b}_{md}^\tau
\]

\begin{align*}
& \la (y_{nd}^m - \sum_k^{K} w_{dk}^m s_{dk}^m z_{nk})^2 \ra = \\
&(y_{nd}^m)^2 - 2y_{nd}^m \sum_k^{K} \la w_{dk}^m s_{dk}^m \ra \la z_{nk} \ra + \sum_k^{K}\sum_j^{K} \la w_{dk}^m s_{dk}^m \ra \la z_{nk} \ra \la w_{dj}^m s_{dj}^m \ra \la z_{nj} \ra = \\
&(y_{nd}^m)^2 - 2y_{nd}^m \sum_k^{K} \la w_{dk}^m s_{dk}^m \ra \la z_{nk} \ra + \sum_k^{K} \la (w_{dk}^m s_{dk}^m)^2 \ra \la z_{nk}^2 \ra + 2\sum_{j > k}^{K} \la w_{dk}^m s_{dk}^m \ra \la w_{dj}^m s_{dj}^m \ra \la z_{nk} \ra \la z_{nj} \ra
\end{align*}


\end{document}
