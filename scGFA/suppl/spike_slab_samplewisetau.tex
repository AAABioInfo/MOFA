\documentclass[10pt, a4paper,openany]{report}

%% Load packages %%
%\usepackage[applemac]{inputenx}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}     % Mathematical writing
\usepackage{amssymb}


%% Load personalised commands %%
\input{utils.tex}


\begin{document}

\section{Spike and Slab with sample-wise noise precision}

\subsection{The model}
We have a data set $\bfY$ of $M$ input views with dimensionality $\bfY^m \in \R^{N \times D_m}$. The sparse group factor analysis model is defined as follows:
\begin{align}
&y_{nd} \approx \Ndist{y_{nd}^m}{\bfw_{d,:}^m\bfz_{n,:},1/\tau_{dn}^m} \\
&w_{dk}^m \approx \theta\Ndist{w_{dk}}{0,1/\alpha_k^m} + (1-\theta)\delta_0(w_{dk}^m)\\
&z_{nk} \approx \Ndist{z_{nk}}{0,1}
\end{align}

The precision of observed data is allowed to vary for samples and features to incorporate a pseudo-data view on the Jaakola bound. This parameter is updated along with the variational parameter of the pseudodata and is not treated as a variational node itself.

\subsection{Changes to standard spike and slab}

The updates affected by this change are the one of $w=s\hat{w}$ and $z$. In the ELB the Bernoulli likelihood of the truly observed data is used.
For all other updates and terms refer to spike\_slab.tex.
\subsection{Joint probability density function:}

\begin{align*}
p(\bfY,\hat{\bfW},\bfS,\bfZ) = &\prod_{m=1}^{M} \prod_{n=1}^{N} \prod_{d=1}^{D_m} \Ndist{y_{nd}^m}{\sum_{k=1}^{K} s_{dk}^m \hat{w}_{dk}^m z_{nk},1/\tau_{nd}} \times \\
&\prod_{d=1}^{D_m}\prod_{k=1}^{K} \Ndist{\hat{w}_{dk}^m}{0,1/\alpha_k^m} \theta^{s_{dk}^m}(1-\theta)^{1-s_{dk}} \times \\
&\prod_{n=1}^{N}\prod_{k=1}^{K} \Ndist{z_{nk}}{0,1}
\end{align*}

Full variational distribution:
\[
q(\hat{\bfW},\bfS,\bfZ) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m,s_{dk}^m) \prod_{k=1}^{K}\prod_{n=1}^{N} q(z_{n,k})
\]

\subsection{Derivation of update equations}
The optimal distribution $\hat{q}_i$ for each variable $\bfx_i$, is the following:
\begin{equation} \label{eq:optimal_q_dist}
\log \hat{q}_i(\bfx_i) = \E_{i \neq j} [\log p(\bfY,\bfX)] + \const
\end{equation}
where $\E_{i \neq j}$ denotes an expectation with respect to the $q$ distributions over all variables $\bfx_j$ except for $\bfx_i$.
The additive constant is set by normalising the distribution $\hat{q}_i(\bfz_i)$:
\[
\hat{q}_i(\bfx_i) = \frac{\exp(\E_{i \neq j}[\log p(\bfY,\bfX)])}{\int \exp(\E_{i \neq j}[\log p(\bfY,\bfX)]) d\bfX}
\]

\subsection*{Latent variables}
Term from the likelihood $p(\bfY|\hat{\bfW},\bfZ,\btau,\bfS)$:
%$\sum_{n=1}^{N}\sum_{d=1}^{D} \Ndist{y_{n,d}}{\sum_{k=1}^{K}\hat{w}_{dk}s_{dk}z_{nk},\tau_d}$:
\begin{align*}
&\sum_{m=1}^{M} \sum_{d=1}^{D=m} \la\tau_{nd}^m\ra \la s_{dk}^m \hat{w}_{dk}^m \ra y_{nd}^m z_{nk}
-\frac{1}{2}\sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_{nd}^m\ra \la (s_{dk}^m\hat{w}_{dk}^m)^2 \ra z_{nk}^2 \\
&-\frac{1}{2}\sum_{m=1}^{M} \frac{1}{2}\sum_{d=1}^{D} \la\tau_{nd}^m\ra \sum_{j \neq k} (\la s_{dj}^m \hat{w}^m_{dj} \ra \la z_{nj} \ra) \la\hat{w}^m_{dk} s_{dk}^m \ra \la z_{nk} \ra + \const
\end{align*}
Term from the prior $p(z_{nk})$:
\[
-\frac{1}{2} z_{nk}^2 + \const
\]
Variational distribution:
\[
q(\bfZ) = \prod_{k=1}^{K} \prod_{n=1}^{N} q(z_{nk}) = \prod_{k=1}^{K} \prod_{n=1}^{N} \Ndist{z_{nk}}{\mu_{z_{nk}},\sigma_{z_{nk}}}
\]
where
\[
\sigma_{z_{nk}}^2 = \Big( \sum_{m=1}^{M} \sum_{d=1}^{D_m} \tau_{nd}^m \la (s_{dk}^m\hat{w}_{dk}^m)^2 \ra + 1 \Big)^{-1}
\]
\[
\mu_{z_{nk}} = \sigma_{z_{nk}}^2 \sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_{nd}^m\ra \la s_{dk}^m\hat{w}_{dk}^m \ra \Big( y_{nd}^m - \sum_{j \neq k} \la s_{dj}^m \hat{w}_{dj}^m \ra \la z_{nj} \ra \Big)
\]

\subsection*{Spike and Slab Weights}
Variational distribution:
\[
q(\hat{\bfW},\bfS) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m,s_{dk}^m) = \prod_{m=1}^{M} \prod_{d=1}^{D_m} \prod_{k=1}^{K} q(\hat{w}_{dk}^m|s_{dk}^m)q(s_{dk}^m)
\]
Update for $q(s_{dk}^m)$:
\[
\gamma_{dk} = q(s_{dk}=1) = \frac{1}{1+\exp(-\lambda_{dk})} =logit^{-1}(\lambda_{dk})
\]
where
\begin{align*}
\lambda_{dk}^m &= \la \log\frac{\theta}{1-\theta} \ra + 0.5 \frac{ \Big( \sum_{n=1}^{N} y_{nd}^m \la z_{nk} \ra \tau_{nd}^m- \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m\ra \sum_{n=1}^{N} \la z_{nk} \ra \la z_{nj} \ra \tau_{nd}^m\Big)^2}{\sum_{n=1}^N \tau_{nd}^m  \la z_{nk}^2\ra +\la\alpha_k^m\ra}\\
 &+ 0.5 \log(\la\alpha_k^m\ra) - 0.5 \log(\sum_{n=1}^N \tau_{nd}^m  \la z_{nk}^2\ra +\la\alpha_k^m\ra)
\end{align*}

Update for $q(\hat{w}_{dk}^m)$:
\begin{align*}
q(\hat{w}_{dk}^m|s_{dk}^m=0) = \Ndist{\hat{w}_{dk}^m}{0,1/\alpha_k^m} \\
q(\hat{w}_{dk}^m|s_{dk}^m=1) = \Ndist{\hat{w}_{dk}^m}{\mu_{w_{dk}^m},\sigma_{w_{dk}^m}^2}
\end{align*}
where
\[
\mu_{w_{dk}^m} = \sigma^2_{w_{dk}^m} \left(\sum_{n=1}^N  \tau_{nd}^m  y_{nd}^m \la z_{nk} \ra -  \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m \ra\sum_{n=1}^N  \tau_{nd}^m \la z_{nk} \ra \la z_{nj} \ra \right)
\]

\[
\sigma^2_{w_{dk}^m} = \frac{1 } { \sum_{n=1}^{N} \tau_{nd}^m \la z_{nk}^2 \ra + \la\alpha_k^m\ra}
\]
Taken together this means that we can update $q(\hat{w}_{dk}^m,s_{dk}^m)$ using:
\[
q(\hat{w}_{dk}^m|s_{dk}^m) \times q(s_{dk}^m) = \Ndist{ \hat{w}_{dk}^m } { s_{dk}^m \mu_{w_{dk}^m}, s_{dk}^m\sigma_{w_{dk}^m}^2 + (1-s_{dk}^m)/\alpha_k^m} \quad \times \quad (\lambda_{dk}^m)^{s_{dk}^m} (1-\lambda_{dk}^m)^{1-s_{dk}}
\]


\subsection{Lower bound}

\subsubsection{Likelihood term}

\[
\log p(y^m_{nd}|Z,W)=y^m_{nd}\sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la z_{nk} \ra - \log\left(1+\exp\left(\sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la z_{nk}\ra\right)\right)
\]


\end{document}
